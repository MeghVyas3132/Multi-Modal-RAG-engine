# ============================================================
# Multi-Modal RAG Engine — Environment Configuration
# Copy this to .env and adjust values for your environment.
# ============================================================

# ── CLIP Model ──────────────────────────────────────────────
CLIP_MODEL_NAME=ViT-H-14
CLIP_PRETRAINED=laion2b_s32b_b79k
# Dimension must match the model. ViT-H-14 = 1024, ViT-L-14 = 768, ViT-B-32 = 512
CLIP_VECTOR_DIM=1024
# Force CPU even if CUDA is available (set to "true" to disable GPU)
FORCE_CPU=false

# ── Qdrant ──────────────────────────────────────────────────
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION=image_vectors
# HNSW tuning — higher m = better recall, more RAM
QDRANT_HNSW_M=16
QDRANT_HNSW_EF_CONSTRUCT=200
# ef at search time — higher = better recall, higher latency
QDRANT_HNSW_EF=128

# ── Redis Cache ─────────────────────────────────────────────
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_CACHE_TTL=3600
REDIS_ENABLED=true

# ── API Server ──────────────────────────────────────────────
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
# Keep workers=1 so the CLIP model is loaded once in RAM.
# Scale horizontally with multiple containers, not workers.

# ── Indexing ────────────────────────────────────────────────
IMAGE_DIR=./data/images
INDEX_BATCH_SIZE=256
INDEX_NUM_WORKERS=4

# ── LLM (optional) ─────────────────────────────────────────
LLM_ENABLED=true
LLM_MODEL=llama-3.3-70b

# ── Cerebras (primary LLM — ~2100 tok/s) ───────────────────
CEREBRAS_API_KEY=your_cerebras_key_here
CEREBRAS_BASE_URL=https://api.cerebras.ai/v1

# ── Groq (fallback LLM — ~300 tok/s) ───────────────────────
GROQ_API_KEY=your_groq_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1

# ── OpenAI (GPT-4o-mini VLM fallback) ──────────────────────
OPENAI_API_KEY=your_openai_key_here

# ── Search Defaults ─────────────────────────────────────────
SEARCH_TOP_K=10
SEARCH_SCORE_THRESHOLD=0.2

# ── V2: Unified Embeddings (Jina-CLIP v2) ──────────────────
UNIFIED_ENABLED=true
UNIFIED_MODEL_NAME=jinaai/jina-clip-v2
UNIFIED_VECTOR_DIM=768
UNIFIED_COLLECTION=unified_vectors
UNIFIED_BATCH_SIZE=32

# ── V2: VLM (SmolVLM-500M) ─────────────────────────────────
VLM_ENABLED=true
VLM_MODEL_NAME=HuggingFaceTB/SmolVLM-500M-Instruct
VLM_CONFIDENCE_THRESHOLD=0.85
VLM_MAX_NEW_TOKENS=256

# ── V2: Modality Router ────────────────────────────────────
ROUTER_ENABLED=true
ROUTER_MODE=heuristic

# ── V2: Web Scraping ───────────────────────────────────────
WEB_SCRAPING_ENABLED=true
JINA_API_KEY=
FIRECRAWL_API_KEY=

# ── V2: Knowledge Graph ────────────────────────────────────
GRAPH_ENABLED=true
GRAPH_MAX_HOPS=2

# ── V2: Semantic Cache ─────────────────────────────────────
SEMANTIC_CACHE_ENABLED=true
SEMANTIC_CACHE_SIMILARITY=0.93

# ── V2: Chunking ───────────────────────────────────────────
CHUNKING_STRATEGY=semantic

# ── V2: Hybrid Retrieval ───────────────────────────────────
HYBRID_SEARCH_ENABLED=true
RERANKER_ENABLED=true
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RETRIEVAL_TOP_K_INITIAL=50

# ── ONNX Runtime (optional, 2-3x faster CPU inference) ────
USE_ONNX=false
ONNX_MODEL_PATH=models/onnx/clip_vit_h14_text_fp32.onnx
ONNX_PROVIDERS=CPUExecutionProvider
ONNX_INTRA_OP_THREADS=4
ONNX_INTER_OP_THREADS=2
ONNX_EXECUTION_MODE=parallel
USE_FP16=false

# ── OpenTelemetry (production observability) ───────────────
OTEL_ENABLED=true
OTEL_ENDPOINT=http://localhost:4317
OTEL_SERVICE_NAME=multimodal-rag-engine
OTEL_SAMPLE_RATE=1.0

# ── Authentication (optional) ──────────────────────────────
AUTH_ENABLED=false
API_KEYS=key1,key2,key3
RATE_LIMIT_ENABLED=false
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW_SECONDS=60
